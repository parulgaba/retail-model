{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirpath = '/Users/parulgaba/Desktop/Capstone-Ethos/ConfidentialData/csvdata/'\n",
    "\n",
    "data_path = '/Users/parulgaba/Desktop/Capstone-Ethos/ethos-retail-model/data/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_north       \n",
    "=> df_north_sanit&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(removed non-stores)         \n",
    "=> df_north_with_market_share &nbsp;&nbsp;&nbsp;(added log So/s)     \n",
    "=> df_north_final&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; (extracted relevant columns)      \n",
    "=> df_north_pareto &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(did pareto analysis)               \n",
    "=> df_north_dummies &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(created dummies)                   \n",
    "=> X,y       &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(seperated independent and dependent variables)                  \n",
    "=> X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "#read existing summary\n",
    "summ_data_file = data_path + 'summary_all_3.csv'\n",
    "store_to_be_removed = data_path +  \"stores_to_be_removed.csv\"\n",
    "summ_data = pd.read_csv(summ_data_file)\n",
    "store_remove=pd.read_csv(store_to_be_removed, header=0,index_col=0)\n",
    "store_remove_list=store_remove.index.to_list()\n",
    "summ_sanit = summ_data[~summ_data['location_code'].isin(store_remove_list)]\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zero_Sales</th>\n",
       "      <th>Pos_Sales</th>\n",
       "      <th>total_count</th>\n",
       "      <th>zero_percent</th>\n",
       "      <th>pos_percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week_agg_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2697016</td>\n",
       "      <td>160587</td>\n",
       "      <td>2857603</td>\n",
       "      <td>94.380360</td>\n",
       "      <td>5.619640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1829999</td>\n",
       "      <td>157869</td>\n",
       "      <td>1987868</td>\n",
       "      <td>92.058376</td>\n",
       "      <td>7.941624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1392893</td>\n",
       "      <td>155109</td>\n",
       "      <td>1548002</td>\n",
       "      <td>89.980052</td>\n",
       "      <td>10.019948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1128427</td>\n",
       "      <td>152656</td>\n",
       "      <td>1281083</td>\n",
       "      <td>88.083832</td>\n",
       "      <td>11.916168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>955528</td>\n",
       "      <td>150297</td>\n",
       "      <td>1105825</td>\n",
       "      <td>86.408609</td>\n",
       "      <td>13.591391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>822986</td>\n",
       "      <td>147925</td>\n",
       "      <td>970911</td>\n",
       "      <td>84.764309</td>\n",
       "      <td>15.235691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>716880</td>\n",
       "      <td>145891</td>\n",
       "      <td>862771</td>\n",
       "      <td>83.090414</td>\n",
       "      <td>16.909586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>650605</td>\n",
       "      <td>143750</td>\n",
       "      <td>794355</td>\n",
       "      <td>81.903557</td>\n",
       "      <td>18.096443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>582342</td>\n",
       "      <td>141964</td>\n",
       "      <td>724306</td>\n",
       "      <td>80.399997</td>\n",
       "      <td>19.600003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>552585</td>\n",
       "      <td>139900</td>\n",
       "      <td>692485</td>\n",
       "      <td>79.797396</td>\n",
       "      <td>20.202604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>510638</td>\n",
       "      <td>138171</td>\n",
       "      <td>648809</td>\n",
       "      <td>78.703902</td>\n",
       "      <td>21.296098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>476147</td>\n",
       "      <td>136529</td>\n",
       "      <td>612676</td>\n",
       "      <td>77.715954</td>\n",
       "      <td>22.284046</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Zero_Sales  Pos_Sales  total_count  zero_percent  pos_percent\n",
       "week_agg_no                                                               \n",
       "2               2697016     160587      2857603     94.380360     5.619640\n",
       "3               1829999     157869      1987868     92.058376     7.941624\n",
       "4               1392893     155109      1548002     89.980052    10.019948\n",
       "5               1128427     152656      1281083     88.083832    11.916168\n",
       "6                955528     150297      1105825     86.408609    13.591391\n",
       "7                822986     147925       970911     84.764309    15.235691\n",
       "8                716880     145891       862771     83.090414    16.909586\n",
       "9                650605     143750       794355     81.903557    18.096443\n",
       "10               582342     141964       724306     80.399997    19.600003\n",
       "11               552585     139900       692485     79.797396    20.202604\n",
       "12               510638     138171       648809     78.703902    21.296098\n",
       "13               476147     136529       612676     77.715954    22.284046"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_df=[]\n",
    "cols=['week_agg_no', 'Zero_Sales', 'Pos_Sales','total_count','zero_percent','pos_percent']\n",
    "for r in range(2,14,1):\n",
    "    #print(r)\n",
    "    weeks_df=summ_sanit.sort_values(['year','week'])['week'].unique()\n",
    "    weeks_df=pd.DataFrame(data=weeks_df,columns=['week'])\n",
    "    weeks_df['period']=(weeks_df.index/r).astype(int)+1\n",
    "    summ_sanit_period=pd.merge(summ_sanit,weeks_df, right_on='week', left_on='week',how='left')\n",
    "    sales_sum_df = summ_sanit_period.groupby(['period','item_no','location_code']).agg({'sales_quantity':'sum'}).reset_index()\n",
    "    zero_sales=len(sales_sum_df[sales_sum_df['sales_quantity'] == 0 ])\n",
    "    pos_sals=len(sales_sum_df[sales_sum_df['sales_quantity'] > 0 ])\n",
    "    total_count=zero_sales+pos_sals\n",
    "    zero_percent=(zero_sales/total_count)*100\n",
    "    pos_percent=(pos_sals/total_count)*100    \n",
    "    lst_df.append([r,zero_sales, pos_sals,total_count,zero_percent,pos_percent])\n",
    "    \n",
    "df1 = pd.DataFrame(lst_df, columns=cols)\n",
    "df1=df1.set_index(\"week_agg_no\") \n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zero_Sales</th>\n",
       "      <th>Pos_Sales</th>\n",
       "      <th>total_count</th>\n",
       "      <th>zero_percent</th>\n",
       "      <th>pos_percent</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>week_agg_no</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1082954</td>\n",
       "      <td>128368</td>\n",
       "      <td>1211322</td>\n",
       "      <td>89.402653</td>\n",
       "      <td>10.597347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>714657</td>\n",
       "      <td>118559</td>\n",
       "      <td>833216</td>\n",
       "      <td>85.770917</td>\n",
       "      <td>14.229083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>531911</td>\n",
       "      <td>110995</td>\n",
       "      <td>642906</td>\n",
       "      <td>82.735423</td>\n",
       "      <td>17.264577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>420283</td>\n",
       "      <td>104803</td>\n",
       "      <td>525086</td>\n",
       "      <td>80.040793</td>\n",
       "      <td>19.959207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>351270</td>\n",
       "      <td>99695</td>\n",
       "      <td>450965</td>\n",
       "      <td>77.892963</td>\n",
       "      <td>22.107037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>296404</td>\n",
       "      <td>95238</td>\n",
       "      <td>391642</td>\n",
       "      <td>75.682383</td>\n",
       "      <td>24.317617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>254508</td>\n",
       "      <td>91698</td>\n",
       "      <td>346206</td>\n",
       "      <td>73.513457</td>\n",
       "      <td>26.486543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>228124</td>\n",
       "      <td>88400</td>\n",
       "      <td>316524</td>\n",
       "      <td>72.071628</td>\n",
       "      <td>27.928372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>200732</td>\n",
       "      <td>85522</td>\n",
       "      <td>286254</td>\n",
       "      <td>70.123736</td>\n",
       "      <td>29.876264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>189231</td>\n",
       "      <td>82649</td>\n",
       "      <td>271880</td>\n",
       "      <td>69.600927</td>\n",
       "      <td>30.399073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>175225</td>\n",
       "      <td>80096</td>\n",
       "      <td>255321</td>\n",
       "      <td>68.629294</td>\n",
       "      <td>31.370706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>162025</td>\n",
       "      <td>77910</td>\n",
       "      <td>239935</td>\n",
       "      <td>67.528706</td>\n",
       "      <td>32.471294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Zero_Sales  Pos_Sales  total_count  zero_percent  pos_percent\n",
       "week_agg_no                                                               \n",
       "2               1082954     128368      1211322     89.402653    10.597347\n",
       "3                714657     118559       833216     85.770917    14.229083\n",
       "4                531911     110995       642906     82.735423    17.264577\n",
       "5                420283     104803       525086     80.040793    19.959207\n",
       "6                351270      99695       450965     77.892963    22.107037\n",
       "7                296404      95238       391642     75.682383    24.317617\n",
       "8                254508      91698       346206     73.513457    26.486543\n",
       "9                228124      88400       316524     72.071628    27.928372\n",
       "10               200732      85522       286254     70.123736    29.876264\n",
       "11               189231      82649       271880     69.600927    30.399073\n",
       "12               175225      80096       255321     68.629294    31.370706\n",
       "13               162025      77910       239935     67.528706    32.471294"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = data_path + 'summary_all_3.csv' # Take periodic summary\n",
    "\n",
    "#data=pd.read_csv(filename)\n",
    "d={'quantity':'count','sales_quantity':'sum','purchase_quantity':'sum','transfer_quantity':'sum'}\n",
    "chunksize = 10 ** 5\n",
    "rows=0\n",
    "df_north = pd.DataFrame()\n",
    "for chunk in pd.read_csv(filename, chunksize=chunksize):\n",
    " \n",
    "    # n=chunk[chunk['state'].isin(['ST03','ST07'])]\n",
    "    df_north=pd.concat([df_north,n])\n",
    "    rows+=chunk.shape[0]\n",
    "print (rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'D:\\\\Backup\\\\ISB CBA\\\\Capstone\\\\summary data\\\\summary_area_code_02.csv'\n",
    "#df_north=pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing location codes which are not stores\n",
    "store_remove=pd.read_excel (\"D:\\\\Backup\\\\ISB CBA\\\\Capstone\\\\stores to be removed.xlsx\", header=0,index_col=0)\n",
    "store_remove_list=store_remove.index.to_list()\n",
    "df_north_sanit = df_north[~df_north['location_code'].isin(store_remove_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['location_code', 'item_no', 'week', 'closing_date', 'brand', 'quantity',\n",
       "       'purchase_quantity', 'transfer_quantity', 'sales_quantity',\n",
       "       'purchase_cost_amount', 'purchase_mrp', 'purchase_date',\n",
       "       'stock_prevailing_mrp', 'store_in', 'product_group_code',\n",
       "       'transfer_cost_amount', 'sales_department', 'days_to_sell',\n",
       "       'num_of_customers', 'total_price', 'line_discount', 'crm_line_discount',\n",
       "       'discount', 'tax', 'cost', 'billing', 'contribution', 'trade_incentive',\n",
       "       'trade_incentive_value', 'total_contribution', 'store_type',\n",
       "       'store_location', 'city_type', 'region', 'state', 'available_quantity',\n",
       "       'week_no', 'year', 'case_size', 'case_size_range', 'gender', 'movement',\n",
       "       'material', 'dial_color', 'strap_type', 'strap_color', 'precious_stone',\n",
       "       'glass', 'case_shape', 'watch_type', 'area_code'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_north_sanit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1087144 682686\n"
     ]
    }
   ],
   "source": [
    "print (len(df_north), len(df_north_sanit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#array=df_north_sanit['location_code'].unique()\n",
    "#df = pd.DataFrame (array)\n",
    "#df.to_excel('D:\\\\Backup\\\\ISB CBA\\\\Capstone\\\\locs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df_north_sanit.groupby(['state','location_code']).agg({'sales_quantity':'sum'})\n",
    "#df.to_excel('D:\\\\Backup\\\\ISB CBA\\\\Capstone\\\\locs.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682686, 51)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_north_sanit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding log S/So"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading market shares\n",
    "market_share=pd.read_excel(data_path + \"market_share_encoded.xlsx\", header=0,index_col=0)\n",
    "\n",
    "#computing market size for each state-period\n",
    "market_sizes=summary_df.groupby(['state','period']).agg({'sales_quantity':'sum'})\n",
    "market_sizes=market_sizes.reset_index()\n",
    "market_sizes=pd.merge(market_sizes,market_share, left_on='state', right_on='SubCode', how='left')#.drop('Attribute_x', axis=1)\n",
    "market_sizes['Market Size']=market_sizes['sales_quantity'].div(market_sizes['Market Share'], axis=0)\n",
    "\n",
    "#computing number of stores per state\n",
    "x=summary_df.groupby(['state','period'])['location_code'].unique()\n",
    "l=[]\n",
    "store_nos=pd.DataFrame()\n",
    "for i in range(len(x)):\n",
    "    l.append([x.index[i][0],x.index[i][1],len(x[i])])\n",
    "cols=['state','period','Store numbers']\n",
    "store_nos = pd.DataFrame(l, columns=cols)\n",
    "\n",
    "#merging market sizes with number of stores per market\n",
    "market_sizes=pd.merge(market_sizes,store_nos, how='inner')\n",
    "\n",
    "#computing market size per store\n",
    "market_sizes['per store market']=market_sizes['Market Size']/market_sizes['Store numbers']\n",
    "\n",
    "#adding market share per store-period to the main data\n",
    "market_sizes=market_sizes[['state','period','per store market']]#extracting only relevant columns from market_sizes\n",
    "merge_cols=['state','period']\n",
    "summary_with_market_shares=pd.merge(summary_df,market_sizes, on=merge_cols,how='inner')\n",
    "\n",
    "#computing So\n",
    "summary_with_market_shares['so']=summary_with_market_shares['per store market']-summary_with_market_shares['sales_quantity']\n",
    "\n",
    "#computing log(S/So) [replacing zeros with 1e-08 so that logs dont create a problem]\n",
    "summary_with_market_shares['so']=summary_with_market_shares['sales_quantity'].replace(0,10**(-5)).div(summary_with_market_shares['so'],axis=0)\n",
    "summary_with_market_shares['so']=np.log(summary_with_market_shares['so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sales_quantity</th>\n",
       "      <th>per store market</th>\n",
       "      <th>so</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [sales_quantity, per store market, so]\n",
       "Index: []"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for NaNs\n",
    "d=df_north_with_market_shares[['sales_quantity','per store market','so']]\n",
    "d[d.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682686, 53)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_north_with_market_shares.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting specific columns\n",
    "\n",
    "col=['location_code', 'item_no', 'state', 'region',\n",
    "       'brand', 'stock_prevailing_mrp', 'store_type', 'store_location', 'city_type',\n",
    "       'available_quantity', 'week_no', 'year', 'case_size_range',\n",
    "       'gender', 'movement', 'material', 'dial_color', 'strap_type',\n",
    "       'strap_color', 'precious_stone', 'glass', 'case_shape', 'watch_type','billing','sales_quantity','so']\n",
    "\n",
    "df_north_final=df_north_with_market_shares.loc[:,col]\n",
    "#df_north_final.fillna(0, inplace=True)\n",
    "\n",
    "df_north_final['item_no']=df_north_final['item_no'].astype(str)\n",
    "#df_north_final['case_size']=df_north_final['case_size'].astype(str)\n",
    "df_north_final['case_shape']=df_north_final['case_shape'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682686, 26)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_north_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function for doing pareto analysis on features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function combines all levels of a categorical features that cummulatively account for ~10% or less by Sales billings into a new level called \"others\". Features with less than 10 levels are not considered for pareto analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pareto(df,cols):\n",
    "    lst=[]\n",
    "    \n",
    "    for col in cols:\n",
    "                \n",
    "        series=df.fillna(0).groupby([col]).agg({'billing':'sum'}).sort_values('billing',ascending=False)\n",
    "        mask=series.cumsum()/series.sum()>0.9 \n",
    "        #nos=mask.value_counts()[1]\n",
    "        mask=mask.iloc[:,0]\n",
    "        levels=len(df_north[col].unique())\n",
    "        \n",
    "        if levels>10:\n",
    "            df[col] = np.where(df[col].isin(series[mask].index),'Other',df[col])         \n",
    "        new_levels=len(df[col].unique())\n",
    "\n",
    "        freq=df[col].value_counts()/df[col].value_counts().sum()*100\n",
    "        freq=freq.round(2)\n",
    "\n",
    "        sale_qty=df.groupby([col]).agg({'sales_quantity':'sum'}).sort_values('sales_quantity',ascending=False)\n",
    "        sale_qty=sale_qty/sale_qty.sum()*100\n",
    "        sale_qty=sale_qty.round(2)\n",
    "        try:\n",
    "            Other_Sales_Qty=sale_qty['sales_quantity']['Other']\n",
    "        except:\n",
    "            Other_Sales_Qty=0\n",
    "        \n",
    "        bill=df.groupby([col]).agg({'billing':'sum'}).sort_values('billing',ascending=False)\n",
    "        bill=bill/bill.sum()*100\n",
    "        bill=bill.round(2)\n",
    "        try:\n",
    "            Other_bill=bill['billing']['Other']\n",
    "        except:\n",
    "            Other_bill=0\n",
    "        \n",
    "        #comparison=mrp.merge(sale_qty, left_index=True, right_index=True)\n",
    "        lst.append([col.upper(),levels, new_levels,Other_bill,Other_Sales_Qty])\n",
    "        #print (\"%s-Originally %d levels,combined %d levels into 'Other'.New Levels %d.By MRP,Other is %2.1f and by sale qty others is %2.1f\"%(col.upper(),levels, levels-new_levels, new_levels,mrp['stock_prevailing_mrp']['Other'],sale_qty['sales_quantity']['Other']))\n",
    "    \n",
    "    cols=['Feature', 'Orig Levels', 'New Levels', 'Other%(Billing)', 'Other%(Sales Qty)']\n",
    "    df1 = pd.DataFrame(lst, columns=cols)\n",
    "    df1=df1.set_index(\"Feature\")\n",
    "    \n",
    "    return df1,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Orig Levels</th>\n",
       "      <th>New Levels</th>\n",
       "      <th>Other%(Billing)</th>\n",
       "      <th>Other%(Sales Qty)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CASE_SIZE_RANGE</th>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>14.18</td>\n",
       "      <td>21.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GENDER</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MOVEMENT</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MATERIAL</th>\n",
       "      <td>62</td>\n",
       "      <td>6</td>\n",
       "      <td>12.58</td>\n",
       "      <td>16.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIAL_COLOR</th>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>11.29</td>\n",
       "      <td>10.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STRAP_TYPE</th>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>10.70</td>\n",
       "      <td>16.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STRAP_COLOR</th>\n",
       "      <td>50</td>\n",
       "      <td>7</td>\n",
       "      <td>14.15</td>\n",
       "      <td>20.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRECIOUS_STONE</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GLASS</th>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CASE_SHAPE</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WATCH_TYPE</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Orig Levels  New Levels  Other%(Billing)  Other%(Sales Qty)\n",
       "Feature                                                                     \n",
       "CASE_SIZE_RANGE           13           6            14.18              21.41\n",
       "GENDER                     3           3             0.00               0.00\n",
       "MOVEMENT                   6           6             0.00               0.00\n",
       "MATERIAL                  62           6            12.58              16.23\n",
       "DIAL_COLOR                52          11            11.29              10.27\n",
       "STRAP_TYPE                65           8            10.70              16.68\n",
       "STRAP_COLOR               50           7            14.15              20.99\n",
       "PRECIOUS_STONE             9           9             0.00               0.00\n",
       "GLASS                      8           7             0.00               0.00\n",
       "CASE_SHAPE                 6           6             0.00               0.00\n",
       "WATCH_TYPE                 6           6             0.00               0.00"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#cols=['brand','case_size', 'case_size_range', 'gender', 'material', 'dial_color', 'strap_type', 'strap_color','precious_stone', 'glass', 'watch_type']\n",
    "cols=['case_size_range', 'gender','movement', 'material', 'dial_color', 'strap_type', 'strap_color','precious_stone', 'glass', 'case_shape', 'watch_type']\n",
    "summary,df_north_pareto=pareto(df_north_final, cols)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(682686, 26)"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_north_pareto.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sorting by week_no and year\n",
    "df_north_pareto=df_north_pareto.sort_values(['week_no', 'year'], ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(683353, 26)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_north_pareto.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dummy variables\n",
    "cols=['brand','state','region', 'store_type', 'store_location', 'city_type',\n",
    "       'case_size_range', 'gender', 'movement', 'material', 'dial_color',\n",
    "       'strap_type', 'strap_color', 'precious_stone', 'glass', 'case_shape',\n",
    "       'watch_type', 'week_no', 'year']\n",
    "df_north_dummies=pd.get_dummies(data=df_north_pareto, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(683353, 240)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_north_dummies.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales_quantity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-3           3\n",
       "-2           8\n",
       "-1         467\n",
       " 0      647588\n",
       " 1       33439\n",
       " 2        1571\n",
       " 3         188\n",
       " 4          35\n",
       " 5          22\n",
       " 6           9\n",
       " 7           2\n",
       " 8           4\n",
       " 9           2\n",
       " 10          5\n",
       " 11          2\n",
       " 12          2\n",
       " 13          1\n",
       " 15          1\n",
       " 22          1\n",
       " 23          1\n",
       " 28          1\n",
       " 135         1\n",
       "Name: sales_quantity, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for checking levels in features\n",
    "a=df_north_pareto.columns\n",
    "xx=a[-2]\n",
    "print(xx)\n",
    "df_north_final[xx].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sales_quantity as the target variable\n",
    "#creating seperate df for independent and dependent features\n",
    "#y=df_north_dummies.loc[:, df_north_dummies.columns == 'sales_quantity']\n",
    "#X=df_north_dummies.drop(columns =['sales_quantity','location_code','item_no','billing','so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for So as the target variable\n",
    "#creating seperate df for independent and dependent features\n",
    "y=df_north_dummies.loc[:, df_north_dummies.columns == 'so']\n",
    "X=df_north_dummies.drop(columns =['sales_quantity','location_code','item_no','billing','sales_quantity', 'so'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking for duplicate column names\n",
    "duplicate_columns = df_north_dummies.columns[df_north_dummies.columns.duplicated()]\n",
    "duplicate_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "#performing train and test split on data\n",
    "\n",
    "split=0.2\n",
    "test=int(len(X)*split)\n",
    "train=len(X)-test\n",
    "X_train=X.head(train)\n",
    "y_train=y.head(train)\n",
    "X_test=X.tail(test)\n",
    "y_test=y.tail(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling using xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Basic modelling without tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.258604\n"
     ]
    }
   ],
   "source": [
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.3, learning_rate = 0.1,max_depth = 5, alpha = 10, n_estimators = 10)\n",
    "xg_reg.fit(X_train,y_train)\n",
    "preds = xg_reg.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7949582649995204"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(y_test,preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tuning hyperparameters    DONT RUN THIS CODE FOR NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'colsample_bytree': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "          'subsample': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "          'colsample_bylevel':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "          'min_child_weight':[1,3,5,7] ,\n",
    "          'reg_lambda':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "          'n_estimator':[3,5,7,10],\n",
    "          'learning_rate': [0.05,0.1,0.15,0.2,0.25,0.3],\n",
    "          'max_depth': [3,4,5,6,7,8,10,12,15], \n",
    "          'reg_alpha': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {'colsample_bytree': [0.1,0.2],\n",
    "          #'subsample': [0.1,0.2,0.3,0.4],\n",
    "          #'colsample_bylevel':[0.1,0.2,0.3,0.4],\n",
    "          #'min_child_weight ':[1,3,5,7] ,\n",
    "#          'reg_lambda':[0.1,0.2,0.3]\n",
    "          #'n_estimator':[3,5,],\n",
    "          #'learning_rate': [0.05,0.1,0.15,0.2,0.25,0.3],\n",
    "          #'max_depth': [3,4,5,6,7,8,10,12,15], \n",
    "          #'reg_alpha': [0.1,0.2,0.3]\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "xg_reg = xgb.XGBRegressor(objective ='reg:squarederror')\n",
    "#scoring='r2'\n",
    "scoring='neg_mean_squared_error'\n",
    "random_search=RandomizedSearchCV(xg_reg,param_distributions=params,scoring=scoring,n_iter=20,cv=5,verbose=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n",
       "             colsample_bynode=1, colsample_bytree=0.8, gamma=0, gpu_id=-1,\n",
       "             importance_type='gain', interaction_constraints='',\n",
       "             learning_rate=0.1, max_delta_step=0, max_depth=12,\n",
       "             min_child_weight=1, min_child_weight =7, missing=nan,\n",
       "             monotone_constraints='()', n_estimator=3, n_estimators=100,\n",
       "             n_jobs=0, num_parallel_tree=1, objective='reg:squarederror',\n",
       "             random_state=0, reg_alpha=0.5, reg_lambda=0.3, scale_pos_weight=1,\n",
       "             subsample=0.2, tree_method='exact', validate_parameters=1,\n",
       "             verbosity=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.2,\n",
       " 'reg_lambda': 0.3,\n",
       " 'reg_alpha': 0.5,\n",
       " 'n_estimator': 3,\n",
       " 'min_child_weight ': 7,\n",
       " 'max_depth': 12,\n",
       " 'learning_rate': 0.1,\n",
       " 'colsample_bytree': 0.8,\n",
       " 'colsample_bylevel': 0.9}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best model based on the output of random_search.best_estimator_\n",
    "best_gb=xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=0.9,\n",
    "             colsample_bynode=1, colsample_bytree=0.8, gamma=0,\n",
    "             importance_type='gain', interaction_constraints='',\n",
    "             learning_rate=0.1, max_delta_step=0, max_depth=12,\n",
    "             min_child_weight =7, missing=np.nan,\n",
    "              n_estimator=3, n_estimators=100,\n",
    "             n_jobs=0, num_parallel_tree=1, objective='reg:squarederror',\n",
    "             random_state=0, reg_alpha=0.5, reg_lambda=0.3, scale_pos_weight=1,\n",
    "             subsample=0.2, tree_method='exact', validate_parameters=1,\n",
    "             verbosity=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "score=cvs(best_gb,X_train,y_train,cv=2,scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.02672306,  0.05306504])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.013170992515409596"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.231691\n"
     ]
    }
   ],
   "source": [
    "best_gb.fit(X_train,y_train)\n",
    "preds = best_gb.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "print(\"RMSE: %f\" % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.231691, R2: 0.059121\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "r2=r2_score(y_test,preds)\n",
    "print(\"RMSE: %.2f, R2: %.2f\" % (rmse,r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics\n",
    "reg_model = LinearRegression()  \n",
    "reg_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = reg_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 0.25, Test R2: -0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "rmse = np.sqrt(mean_squared_error(y_test, preds))\n",
    "r2=r2_score(y_test,preds)\n",
    "print(\"Test RMSE: %.2f, Test R2: %.2f\" % (rmse,r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    647604\n",
       "1     33414\n",
       "Name: sales_quantity, dtype: int64"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retaining only 0 and 1 for sales_quantity\n",
    "df_north_pareto_classifier = df_north_pareto[df_north_pareto['sales_quantity'].isin([0,1])]\n",
    "df_north_pareto_classifier['sales_quantity'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dummy variables\n",
    "cols=['brand','state','region', 'store_type', 'store_location', 'city_type',\n",
    "       'case_size_range', 'gender', 'movement', 'material', 'dial_color',\n",
    "       'strap_type', 'strap_color', 'precious_stone', 'glass', 'case_shape',\n",
    "       'watch_type', 'week_no', 'year']\n",
    "df_north_dummies_clf=pd.get_dummies(data=df_north_pareto_classifier, columns=cols)\n",
    "\n",
    "#for sales_quantity as the target variable\n",
    "#creating seperate df for independent and dependent features\n",
    "y_clf=df_north_dummies_clf.loc[:, df_north_dummies_clf.columns == 'sales_quantity']\n",
    "X_clf=df_north_dummies_clf.drop(columns =['sales_quantity','location_code','item_no','billing','so'])\n",
    "\n",
    "#performing train and test split on data\n",
    "\n",
    "split=0.2\n",
    "test_clf=int(len(X_clf)*split)\n",
    "train_clf=len(X_clf)-test_clf\n",
    "X_train_clf=X_clf.head(train_clf)\n",
    "y_train_clf=y_clf.head(train_clf)\n",
    "X_test_clf=X_clf.tail(test_clf)\n",
    "y_test_clf=y_clf.tail(test_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balancing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U imbalanced-learn\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=11915027)\n",
    "X_bal_clf, y_bal_clf = ros.fit_resample(X_train_clf, y_train_clf)\n",
    "#classifier(X_bal,y_bal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "544815 1035492 544815 1035492\n"
     ]
    }
   ],
   "source": [
    "print (len(X_train_clf),len(X_bal_clf),len(y_train_clf),len(y_bal_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Before Balancing</th>\n",
       "      <th>After Balancing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>517746</td>\n",
       "      <td>517746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27069</td>\n",
       "      <td>517746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Before Balancing  After Balancing\n",
       "0            517746           517746\n",
       "1             27069           517746"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts=y_train_clf.iloc[:, 0].value_counts().to_frame(name='Before Balancing')\n",
    "count1= pd.DataFrame(np.bincount(y_bal_clf.iloc[:, 0]))\n",
    "counts['After Balancing'] = count1\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining function for computing confusion metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining function for computing confusion metrics\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "def cfm(y_test,y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tnp=tn/(tn+fp)*100\n",
    "    fpp=fp/(tn+fp)*100\n",
    "    fnp=fn/(fn+tp)*100\n",
    "    tpp=tp/(fn+tp)*100\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    accuracy=metrics.accuracy_score(y_test, y_pred)*100\n",
    "    return accuracy,tnp,fpp,fnp,tpp,roc_auc\n",
    "    #return accuracy,tn,fp,fn,tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ignore this function for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "def classifier(x_train, x_test, y_train, y_test):\n",
    "\n",
    "    #x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=11915027)\n",
    "    #x_train_bal, x_test_bal, y_train_bal, y_test_bal = train_test_split(X_bal, y_bal, test_size=0.30, random_state=11915027)\n",
    "\n",
    "    output_list=[]\n",
    "\n",
    "    arr=np.bincount(y_train.iloc[:, 0])\n",
    "    a=np.where( arr== np.max(arr))\n",
    "    a=a[0][0]\n",
    "    np.count_nonzero(y_test == a)\n",
    "    y_pred=np.full(len(y_test),a)\n",
    "    #baseline_accuracy=np.count_nonzero(y_test == a)/len(y_test)\n",
    "    #baseline_accuracy\n",
    "    accuracy,tnp,fpp,fnp,tpp = cfm(y_test,y_pred)\n",
    "    output_list.append(['Majority Class',accuracy,tnp,fpp,fnp,tpp])\n",
    "\n",
    "    log_model = LogisticRegression(solver='newton-cg',multi_class='multinomial')\n",
    "    log_model.fit(x_train,y_train)\n",
    "    y_pred = log_model.predict(x_test)\n",
    "    #log_accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    #log_accuracy\n",
    "    accuracy,tnp,fpp,fnp,tpp = cfm(y_test,y_pred)\n",
    "    output_list.append(['Log Regression',accuracy,tnp,fpp,fnp,tpp])\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(x_train,y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    #nb_accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    accuracy,tnp,fpp,fnp,tpp = cfm(y_test,y_pred)\n",
    "    output_list.append(['Naive Bayes',accuracy,tnp,fpp,fnp,tpp])\n",
    "\n",
    "    d_tree_clfr = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf=25)\n",
    "    d_tree_clfr.fit(x_train, y_train)\n",
    "    y_pred= d_tree_clfr.predict(x_test)\n",
    "    #dt_accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    accuracy,tnp,fpp,fnp,tpp = cfm(y_test,y_pred)\n",
    "    output_list.append(['Decision Tree',accuracy,tnp,fpp,fnp,tpp])\n",
    "\n",
    "    lda_clf = LinearDiscriminantAnalysis()\n",
    "    lda_clf.fit(x_train, y_train)\n",
    "    y_pred= lda_clf.predict(x_test)\n",
    "    #lda_accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "    accuracy,tnp,fpp,fnp,tpp = cfm(y_test,y_pred)\n",
    "    output_list.append(['LDA',accuracy,tnp,fpp,fnp,tpp])\n",
    "\n",
    "    #output_list.append([baseline_accuracy,log_accuracy,nb_accuracy,dt_accuracy,lda_accuracy])\n",
    "    #output=pd.DataFrame(output_list,columns=['Baseline','Log Regression','Naive Bayes','Decision Tree','LDA'])\n",
    "    output=pd.DataFrame(output_list,columns=['Model','Accuracy%','TN%', 'FP%', 'FN%', 'TP%'])\n",
    "\n",
    "    output=output.round(2)\n",
    "    #accuracy,tnp,fpp,fnp,tpp = cfm(y_test,y_pred)\n",
    "    #print (accuracy,tnp,fpp,fnp,tpp)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing baseline accuracy if all predictions were zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy%</th>\n",
       "      <th>TN%</th>\n",
       "      <th>FP%</th>\n",
       "      <th>FN%</th>\n",
       "      <th>TP%</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Majority Class</td>\n",
       "      <td>95.34</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Accuracy%    TN%  FP%    FN%  TP%  AUC\n",
       "0  Majority Class      95.34  100.0  0.0  100.0  0.0  0.5"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# computing baseline accuracy if all predictions were zero\n",
    "\n",
    "output_list=[]\n",
    "arr=np.bincount(y_train_clf.iloc[:, 0])\n",
    "a=np.where( arr== np.max(arr))\n",
    "a=a[0][0]\n",
    "np.count_nonzero(y_test_clf == a)\n",
    "y_pred=np.full(len(y_test_clf),a)\n",
    "#baseline_accuracy=np.count_nonzero(y_test == a)/len(y_test)\n",
    "#baseline_accuracy\n",
    "\n",
    "accuracy,tnp,fpp,fnp,tpp,roc_auc = cfm(y_test_clf,y_pred)\n",
    "output_list.append(['Majority Class',accuracy,tnp,fpp,fnp,tpp,roc_auc])\n",
    "output=pd.DataFrame(output_list,columns=['Model','Accuracy%','TN%', 'FP%', 'FN%', 'TP%','AUC'])\n",
    "output.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy%</th>\n",
       "      <th>TN%</th>\n",
       "      <th>FP%</th>\n",
       "      <th>FN%</th>\n",
       "      <th>TP%</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Majority Class</td>\n",
       "      <td>95.34</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>95.34</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>100.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>95.35</td>\n",
       "      <td>99.86</td>\n",
       "      <td>0.14</td>\n",
       "      <td>96.85</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model  Accuracy%     TN%   FP%     FN%   TP%   AUC\n",
       "0  Majority Class      95.34  100.00  0.00  100.00  0.00  0.50\n",
       "1   Decision Tree      95.34  100.00  0.00  100.00  0.00  0.50\n",
       "2   Decision Tree      95.35   99.86  0.14   96.85  3.15  0.52"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_tree_clfr = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf=100)\n",
    "d_tree_clfr.fit(X_train_clf, y_train_clf)\n",
    "y_pred= d_tree_clfr.predict(X_test_clf)\n",
    "#dt_accuracy=metrics.accuracy_score(y_test, y_pred)\n",
    "accuracy,tnp,fpp,fnp,tpp,roc_auc = cfm(y_test_clf,y_pred)\n",
    "output_list.append(['Decision Tree',accuracy,tnp,fpp,fnp,tpp,roc_auc])\n",
    "\n",
    "output=pd.DataFrame(output_list,columns=['Model','Accuracy%','TN%', 'FP%', 'FN%', 'TP%','AUC'])\n",
    "output.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy%</th>\n",
       "      <th>TN%</th>\n",
       "      <th>FP%</th>\n",
       "      <th>FN%</th>\n",
       "      <th>TP%</th>\n",
       "      <th>AUC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SizeThreshold</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>70.42</td>\n",
       "      <td>70.77</td>\n",
       "      <td>29.23</td>\n",
       "      <td>36.72</td>\n",
       "      <td>63.28</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>69.45</td>\n",
       "      <td>69.66</td>\n",
       "      <td>30.34</td>\n",
       "      <td>34.93</td>\n",
       "      <td>65.07</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>68.26</td>\n",
       "      <td>68.37</td>\n",
       "      <td>31.63</td>\n",
       "      <td>34.01</td>\n",
       "      <td>65.99</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>68.46</td>\n",
       "      <td>68.59</td>\n",
       "      <td>31.41</td>\n",
       "      <td>34.17</td>\n",
       "      <td>65.83</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>68.40</td>\n",
       "      <td>68.50</td>\n",
       "      <td>31.50</td>\n",
       "      <td>33.52</td>\n",
       "      <td>66.48</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>68.12</td>\n",
       "      <td>68.19</td>\n",
       "      <td>31.81</td>\n",
       "      <td>33.18</td>\n",
       "      <td>66.82</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>68.29</td>\n",
       "      <td>68.35</td>\n",
       "      <td>31.65</td>\n",
       "      <td>32.96</td>\n",
       "      <td>67.04</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>68.70</td>\n",
       "      <td>68.80</td>\n",
       "      <td>31.20</td>\n",
       "      <td>33.27</td>\n",
       "      <td>66.73</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>67.99</td>\n",
       "      <td>68.06</td>\n",
       "      <td>31.94</td>\n",
       "      <td>33.62</td>\n",
       "      <td>66.38</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>325</th>\n",
       "      <td>67.64</td>\n",
       "      <td>67.67</td>\n",
       "      <td>32.33</td>\n",
       "      <td>33.02</td>\n",
       "      <td>66.98</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>67.09</td>\n",
       "      <td>67.08</td>\n",
       "      <td>32.92</td>\n",
       "      <td>32.83</td>\n",
       "      <td>67.17</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375</th>\n",
       "      <td>66.96</td>\n",
       "      <td>66.95</td>\n",
       "      <td>33.05</td>\n",
       "      <td>32.94</td>\n",
       "      <td>67.06</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>66.98</td>\n",
       "      <td>66.95</td>\n",
       "      <td>33.05</td>\n",
       "      <td>32.23</td>\n",
       "      <td>67.77</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>66.46</td>\n",
       "      <td>66.36</td>\n",
       "      <td>33.64</td>\n",
       "      <td>31.51</td>\n",
       "      <td>68.49</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450</th>\n",
       "      <td>66.00</td>\n",
       "      <td>65.87</td>\n",
       "      <td>34.13</td>\n",
       "      <td>31.41</td>\n",
       "      <td>68.59</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>66.08</td>\n",
       "      <td>65.97</td>\n",
       "      <td>34.03</td>\n",
       "      <td>31.62</td>\n",
       "      <td>68.38</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>66.55</td>\n",
       "      <td>66.48</td>\n",
       "      <td>33.52</td>\n",
       "      <td>31.93</td>\n",
       "      <td>68.07</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>67.06</td>\n",
       "      <td>67.00</td>\n",
       "      <td>33.00</td>\n",
       "      <td>31.73</td>\n",
       "      <td>68.27</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>550</th>\n",
       "      <td>66.45</td>\n",
       "      <td>66.34</td>\n",
       "      <td>33.66</td>\n",
       "      <td>31.25</td>\n",
       "      <td>68.75</td>\n",
       "      <td>0.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>66.51</td>\n",
       "      <td>66.42</td>\n",
       "      <td>33.58</td>\n",
       "      <td>31.55</td>\n",
       "      <td>68.45</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>65.98</td>\n",
       "      <td>65.87</td>\n",
       "      <td>34.13</td>\n",
       "      <td>31.62</td>\n",
       "      <td>68.38</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>625</th>\n",
       "      <td>65.99</td>\n",
       "      <td>65.85</td>\n",
       "      <td>34.15</td>\n",
       "      <td>31.22</td>\n",
       "      <td>68.78</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>650</th>\n",
       "      <td>66.48</td>\n",
       "      <td>66.40</td>\n",
       "      <td>33.60</td>\n",
       "      <td>32.03</td>\n",
       "      <td>67.97</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>65.86</td>\n",
       "      <td>65.72</td>\n",
       "      <td>34.28</td>\n",
       "      <td>31.44</td>\n",
       "      <td>68.56</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>65.34</td>\n",
       "      <td>65.15</td>\n",
       "      <td>34.85</td>\n",
       "      <td>30.83</td>\n",
       "      <td>69.17</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>725</th>\n",
       "      <td>65.52</td>\n",
       "      <td>65.35</td>\n",
       "      <td>34.65</td>\n",
       "      <td>31.03</td>\n",
       "      <td>68.97</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>750</th>\n",
       "      <td>65.52</td>\n",
       "      <td>65.34</td>\n",
       "      <td>34.66</td>\n",
       "      <td>30.75</td>\n",
       "      <td>69.25</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>65.24</td>\n",
       "      <td>65.02</td>\n",
       "      <td>34.98</td>\n",
       "      <td>30.29</td>\n",
       "      <td>69.71</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800</th>\n",
       "      <td>64.60</td>\n",
       "      <td>64.34</td>\n",
       "      <td>35.66</td>\n",
       "      <td>30.12</td>\n",
       "      <td>69.88</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>825</th>\n",
       "      <td>64.54</td>\n",
       "      <td>64.27</td>\n",
       "      <td>35.73</td>\n",
       "      <td>29.98</td>\n",
       "      <td>70.02</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850</th>\n",
       "      <td>64.65</td>\n",
       "      <td>64.39</td>\n",
       "      <td>35.61</td>\n",
       "      <td>30.04</td>\n",
       "      <td>69.96</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>875</th>\n",
       "      <td>64.73</td>\n",
       "      <td>64.46</td>\n",
       "      <td>35.54</td>\n",
       "      <td>29.55</td>\n",
       "      <td>70.45</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900</th>\n",
       "      <td>64.79</td>\n",
       "      <td>64.54</td>\n",
       "      <td>35.46</td>\n",
       "      <td>30.13</td>\n",
       "      <td>69.87</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>64.57</td>\n",
       "      <td>64.30</td>\n",
       "      <td>35.70</td>\n",
       "      <td>29.88</td>\n",
       "      <td>70.12</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>950</th>\n",
       "      <td>64.29</td>\n",
       "      <td>63.97</td>\n",
       "      <td>36.03</td>\n",
       "      <td>29.27</td>\n",
       "      <td>70.73</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>64.64</td>\n",
       "      <td>64.36</td>\n",
       "      <td>35.64</td>\n",
       "      <td>29.61</td>\n",
       "      <td>70.39</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Accuracy%    TN%    FP%    FN%    TP%   AUC\n",
       "SizeThreshold                                             \n",
       "100                70.42  70.77  29.23  36.72  63.28  0.67\n",
       "125                69.45  69.66  30.34  34.93  65.07  0.67\n",
       "150                68.26  68.37  31.63  34.01  65.99  0.67\n",
       "175                68.46  68.59  31.41  34.17  65.83  0.67\n",
       "200                68.40  68.50  31.50  33.52  66.48  0.67\n",
       "225                68.12  68.19  31.81  33.18  66.82  0.68\n",
       "250                68.29  68.35  31.65  32.96  67.04  0.68\n",
       "275                68.70  68.80  31.20  33.27  66.73  0.68\n",
       "300                67.99  68.06  31.94  33.62  66.38  0.67\n",
       "325                67.64  67.67  32.33  33.02  66.98  0.67\n",
       "350                67.09  67.08  32.92  32.83  67.17  0.67\n",
       "375                66.96  66.95  33.05  32.94  67.06  0.67\n",
       "400                66.98  66.95  33.05  32.23  67.77  0.67\n",
       "425                66.46  66.36  33.64  31.51  68.49  0.67\n",
       "450                66.00  65.87  34.13  31.41  68.59  0.67\n",
       "475                66.08  65.97  34.03  31.62  68.38  0.67\n",
       "500                66.55  66.48  33.52  31.93  68.07  0.67\n",
       "525                67.06  67.00  33.00  31.73  68.27  0.68\n",
       "550                66.45  66.34  33.66  31.25  68.75  0.68\n",
       "575                66.51  66.42  33.58  31.55  68.45  0.67\n",
       "600                65.98  65.87  34.13  31.62  68.38  0.67\n",
       "625                65.99  65.85  34.15  31.22  68.78  0.67\n",
       "650                66.48  66.40  33.60  32.03  67.97  0.67\n",
       "675                65.86  65.72  34.28  31.44  68.56  0.67\n",
       "700                65.34  65.15  34.85  30.83  69.17  0.67\n",
       "725                65.52  65.35  34.65  31.03  68.97  0.67\n",
       "750                65.52  65.34  34.66  30.75  69.25  0.67\n",
       "775                65.24  65.02  34.98  30.29  69.71  0.67\n",
       "800                64.60  64.34  35.66  30.12  69.88  0.67\n",
       "825                64.54  64.27  35.73  29.98  70.02  0.67\n",
       "850                64.65  64.39  35.61  30.04  69.96  0.67\n",
       "875                64.73  64.46  35.54  29.55  70.45  0.67\n",
       "900                64.79  64.54  35.46  30.13  69.87  0.67\n",
       "925                64.57  64.30  35.70  29.88  70.12  0.67\n",
       "950                64.29  63.97  36.03  29.27  70.73  0.67\n",
       "975                64.64  64.36  35.64  29.61  70.39  0.67"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "lst=[]\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=11915027)\n",
    "#x_train_bal, x_test_bal, y_train_bal, y_test_bal = train_test_split(X_bal, y_bal, test_size=0.30, random_state=11915027)\n",
    "for i in range(100,1000,25): \n",
    "    d_tree_clfr = DecisionTreeClassifier(criterion = 'entropy', min_samples_leaf=i)\n",
    "    #Apply classifier on training dataset\n",
    "    d_tree_clfr.fit(X_bal_clf, y_bal_clf)\n",
    "    y_pred = d_tree_clfr.predict(X_test_clf)\n",
    "    accuracy,tnp,fpp,fnp,tpp,roc_auc = cfm(y_test_clf,y_pred)\n",
    "    lst.append([i,accuracy,tnp,fpp,fnp,tpp,roc_auc])\n",
    "cols=['SizeThreshold','Accuracy%','TN%', 'FP%', 'FN%', 'TP%','AUC']\n",
    "df = pd.DataFrame(lst, columns=cols)\n",
    "df=df.set_index(\"SizeThreshold\")\n",
    "df.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_bal_clf, y_bal_clf)\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
    "    max_features=None, max_leaf_nodes=None,\n",
    "    min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "     min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "     presort=False, random_state=None, splitter='best')\n",
    "y_pred = dt.predict(X_test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'min_samples_leaf': 0.1142857142857143, 'max_depth': 15.0, 'criterion': 'gini'}\n",
      "Best score is 0.6193861791108246\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV \n",
    "param_dist = {\"max_depth\": np.linspace(1, 32, 32, endpoint=True), \n",
    "              \"min_samples_leaf\": np.linspace(0.05, 0.5, 50, endpoint=True), \n",
    "              \"criterion\": [\"gini\", \"entropy\"]} \n",
    "tree = DecisionTreeClassifier() \n",
    "tree_cv = RandomizedSearchCV(tree, param_dist, cv = 5,scoring='roc_auc') \n",
    "tree_cv.fit(X_train_clf, y_train_clf) \n",
    "print(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(tree_cv.best_score_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Decision Tree Parameters: {'subsample': 0.9, 'min_samples_leaf': 0.1510204081632653, 'max_depth': 6, 'learning_rate': 0.25}\n",
      "Best score is 0.6548648277071013\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as gbc\n",
    "params = {\"max_depth\": np.linspace(1, 32, 32, endpoint=True),\n",
    "          'subsample': [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n",
    "          \"min_samples_leaf\": np.linspace(0.05, 0.5, 50, endpoint=True),         \n",
    "          #'n_estimator':[3,5,7,10],\n",
    "          'learning_rate': [0.05,0.1,0.15,0.2,0.25,0.3],\n",
    "          'max_depth': [3,4,5,6,7,8,10,12,15], \n",
    "          \n",
    "         }\n",
    "\n",
    "#learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, \n",
    "#min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, \n",
    "#min_impurity_split=None, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, \n",
    "#warm_start=False,presort='deprecated', validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, \n",
    "#ccp_alpha=0.0)[source]\n",
    "gb = gbc() \n",
    "gb_cv = RandomizedSearchCV(gb, params, cv = 5,scoring='roc_auc',verbose=1) \n",
    "gb_cv.fit(X_train_clf, y_train_clf.iloc[:, 0]) \n",
    "print(\"Tuned Gradient Boost Parameters: {}\".format(gb_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(gb_cv.best_score_)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Random Forest Parameters: {'min_samples_leaf': 0.05918367346938776, 'max_depth': 15.0, 'criterion': 'entropy'}\n",
      "Best score is 0.6435627568690562\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rfc\n",
    "params = {\"max_depth\": np.linspace(1, 80, 80, endpoint=True),\n",
    "          \"min_samples_leaf\": np.linspace(0.05, 0.5, 50, endpoint=True),         \n",
    "          #'n_estimator':[3,5,7,10],         \n",
    "          \"criterion\": [\"gini\", \"entropy\"]\n",
    "         }\n",
    "\n",
    "#class sklearn.ensemble.RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, \n",
    "#min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', \n",
    "#max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, \n",
    "#n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)[source]\n",
    "rf = rfc() \n",
    "rf_cv = RandomizedSearchCV(rf, params, cv = 5,scoring='roc_auc',verbose=1) \n",
    "rf_cv.fit(X_train_clf, y_train_clf.iloc[:, 0]) \n",
    "print(\"Tuned Random Forest Parameters: {}\".format(rf_cv.best_params_)) \n",
    "print(\"Best score is {}\".format(rf_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['location_code', 'item_no', 'state', 'region', 'brand',\n",
       "       'stock_prevailing_mrp', 'store_type', 'store_location', 'city_type',\n",
       "       'available_quantity', 'week_no', 'year', 'case_size_range', 'gender',\n",
       "       'movement', 'material', 'dial_color', 'strap_type', 'strap_color',\n",
       "       'precious_stone', 'glass', 'case_shape', 'watch_type', 'billing',\n",
       "       'sales_quantity', 'so'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_north_pareto.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_clf.columns.to_frame().to_excel('D:\\\\Backup\\\\ISB CBA\\\\Capstone\\\\columns.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmodes.kprototypes import KPrototypes\n",
    "kproto = KPrototypes(n_clusters=15, init='Cao', verbose=2)\n",
    "clusters = kproto.fit_predict(X_train_clf, categorical=[1, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
